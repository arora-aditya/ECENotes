---
title: 'Chapter 7 - Properties of Expectation<br />'
created: '2019-03-2921:15:00.000Z'
author: 'Aditya Arora'
---
## Introduction

**Discrete**: $E[X] = \sum_xxp(x)$\
**Continuous**: $E[X] = \int_{-\infty}^{\infty}xf(x)dx$

## Expectation of Sums of Random Variables

General Formula: functions $g(x)$ of random variables

**Discrete**: $E[X] = \sum_xg(x)p(x)$\
**Continuous**: $E[X] = \int_{-\infty}^{\infty}g(x)f(x)dx$

Two random variables

$$E[g(X, Y)] = 
\begin{cases}
  \sum_{x,y}g(x, y)p(x, y) & \text{Discrete Case}\\
    \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x)f(x)dxdy & \text{Continuous Case}
\end{cases}$$

In particular, if $g(X, Y) = X+Y$, then 
$$\begin{aligned}
E[X+Y] &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}(x+y)f(x,y)dxdy\\
&= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}xf(x,y)dxdy + \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}yf(x,y)dxdy\\
&= \int_{-\infty}^{\infty}x\Bigg\{\underbrace{\int_{-\infty}^{\infty}f(x,y)dy}_{f_X(x)}\Bigg\}dx + \int_{-\infty}^{\infty}y\Bigg\{\underbrace{\int_{-\infty}^{\infty}f(y,x)dx}_{f_Y(y)}\Bigg\}dy\\
&= E[X] + E[Y]
\end{aligned}$$

By induction, generally $E[X_1+X_2+\dots + X_n]=E[X_1] + E[X_2] +\dots + E[X_n]$

## <sub><sub><sub><sub><sub><sub>not found in notes</sub></sub></sub></sub></sub></sub>
\
\

## Covariance, Variance of Sums and Correlations

**Proposition 4.1**: If $X$ and $Y$ are independent, then for any functions $g$ and $h$, $E[g(X)h(Y)] = E[g(X)]E[h(Y)]$

_Proof_
$$\begin{aligned}
E[g(X)h(Y)] &= \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x)h(y)\underbrace{f(x,y)}_{f_X(x)\cdot f_Y(y)}dxdy\\
&= \int_{-\infty}^{\infty}\underbrace{g(x)f_X(x)}_{E[g(X)]}dx
\int_{-\infty}^{\infty}\underbrace{h(y)f_Y(y)}_{E[h(Y)]}dy\\
&= E[g(X)]E[h(Y)]
\end{aligned}$$

**Definition**: $cov(X, Y) = E[(X - EX)(Y-EY)]$
$$cov(X,X) = E[(X-EX)(X-EX)] = E[(X-EX)^2] = Var(X)$$

We have known that $Var(X) = E[X^2] - (E[X])^2$

$$\begin{aligned}
Cov(X, Y) &= E[(X - EX)(Y - EY)]\\
&= E[XY] - \underbrace{E[X\cdot EY]}_{E[Y]\cdot E[X]} - \underbrace{E[Y\cdot EX]}_{E[Y]\cdot E[X]} + - \underbrace{E[EX\cdot EY]}_{E[Y]\cdot E[X]}\\
&= E[XY] - E[X]\cdot E[Y]
\end{aligned}$$


If $X$ and $Y$ are independent, then $cov(X, Y) = E[XY] - E[X]\cdot E[Y] = E[X]\cdot E[Y] -  E[X]\cdot E[Y] = 0$

Therefore, independence implies the zero covariance, but the converse doesn't really hold. 

**Proposition 4.2**

1. $cov(X, Y) = cov(Y, X)$
2. $cov(X,X) = Var(X)$
3. $cov(aX, Y) = a\cdot cov(X, Y)$
4. $cov(\sum_{i=1}^{n}X, \sum_{j=1}^{m}Y_j) = \sum_{i=1}^{n}\sum_{j=1}^{m}cov(X_i, Y_j)$

We have known that $E[\sum_{i=1}^{n}X_i] = \sum_{i=1}^{n}E[X_i]$, what's $Var(\sum_{i=1}^{n}X_i)$

$$\begin{aligned}
Var(\sum_{i=1}^{n}X_i) &= cov(\sum_{i=1}^{n}X_i, \sum_{j=1}^{n}X_j)\\
&= \sum_{i=1}^{n}\sum_{j=1}^{n}Cov(X_i, X_j)\\
&= \sum_{i=j=1}^{n}Cov(X_i, X_j) + \sum_{i\neq j}^{n}Cov(X_i, X_j)\\
&= \sum_{i=1}^{n}Var(X_i) + \sum_{i\neq j}^{n}Cov(X_i, X_j)\\
\end{aligned}$$
If $X_i (i=1,\dots, n)$ are independent, thens $Cov(X_i, X_j) = 0\;\; \forall\;\; i \neq j$
$$\therefore Var(\sum_{i=1}^{n}X_i) = \sum_{i=1}^{n}Var(X_i)$$

## Conditional Expectation

### Definitions

**Discrete**: Conditional PMF: $P_{X|Y}(x|y) = P\{X=x, Y=y\} = \frac{P(x,y)}{P_Y(y)}$\
Conditional Expectation: $E[X|Y=y] = \sum_xP\{X=x|Y=y\} = \sum_xP_{X|Y}\{x|y\}$\

**Continuous**: Conditional PMF: $f_{X|Y}(x|y) = \frac{f(x,y)}{f_Y(y)}$\
Conditional Expectation: $E[X|Y=y] = \sum_xP\{X=x|Y=y\} = \int_{-\infty}^{\infty}f_{X|Y}\{x|y\}dx$\

Similar formulas

$$
E[g(X)|Y=y] = \begin{cases}
\sum_xg(x)P_{X|Y}(x|y) & \text{Discrete}\\
\int_{-\infty}^{\infty} g(x)f_{X|Y}(x|y)dx & \text{Continuous}\\
\end{cases} 
$$

and $E[\sum_{i=1}^{n}X_i|Y=y] = \sum_{i=1}^nE[X_i|Y=y]$

### Computing Expectations by conditioning

Note that $E[X|Y]$ is a random variable since $$E[X|Y] = \begin{cases}
E[X|Y=y_1] & \text{when } Y = y_1\\
E[X|Y=y_2] & \text{when } Y = y_2\\
& \vdots
\end{cases}$$

**Proposition 5.1**: $E[X] = E[E[X|Y]]$
$$
E[X] = \begin{cases}
\sum_yE[X|Y=y]P\{Y=y\} & \text{Discrete}\\
\int_{-\infty}^{\infty} \sum_yE[X|Y=y]f_{Y}(y)dy & \text{Continuous}\\
\end{cases} 
$$

## <sub><sub><sub><sub><sub><sub>not found in notes</sub></sub></sub></sub></sub></sub>
\
\

## Moment Generating Functions

The moment generating function (MGF) $M(t)$ of the random variable $X$ is defined for all real values of $t$ by 

$$
M(t) = E[e^{tX}] = 
\begin{cases}
\sum_xe^{tx}p(x) &  \text{Discrete}\\
\int_{-\infty}^{\infty} e^{tx}f(x)dx & \text{Continuous}\\
\end{cases}
$$


It is called MGF because of all the moments of $X$ can be obtained by successively differentiating $M(t)$ and then evaluate the result at $t = 0$

In general, $M^{(n)}(t) = E[X^ne^{tX}], n \geq 0$, $M^{(n)}(0) = E[X^n]\sim n^{th}$ moment of $X$


| X                     	| $M(t)$                                     	|
|-----------------------	|------------------------------------------	|
| Binomial              	| $(pe^t + 1 - p)^n$                         	|
| Poisson($\lambda$)      	| $exp(\lambda(e^t - 1))$                    	|
| Exponential ($\lambda$)	| $\frac{\lambda}{\lambda - t}, t < \lambda$ 	|
| $N(\mu, \sigma^2)$     	| $exp(\mu t + \frac{\sigma^2 t^2}{2})$      	|


**Property 1**: If $X$ and $Y$ are independent ($f_{X+Y} = f_X * f_Y$), then

$$M_{X+Y}(t) = E[e^{t(X+Y)}] = E[e^{tX}e^{tY}] = E[e^{tX}]\cdot E[e^{tY}] = M_X(t)\cdot M_Y(t)$$

**Property 2**: If $M_X(t)$ exists and is finite in some region about $t=0$, then the distribution of $X$ is uniquely determined

| X, Y (independent)      	| $X+Y$                                            	| $M(t)$                                                            	|
|-------------------------	|------------------------------------------------	|-------------------------------------------------------------------	|
| Binomial                	| $\sim binomial(n+m, P)$                          	| $(pe^t + 1 - p)^{n+m}$                                            	|
| Poisson($\lambda$)      	| $\sim Poisson(\lambda_1 + \lambda_2)$            	| $exp((\lambda_1 + \lambda_2)(e^t - 1))$                           	|
| Exponential ($\lambda$) 	| $\not\sim exponential$                           	| $\frac{\lambda_1\cdot\lambda_2}{(\lambda_1 - t)(\lambda_2 - t)}$  	|
| $N(\mu, \sigma^2)$      	| $\sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$ 	| $exp((\mu_1 + mu_2) t + \frac{(\sigma_1^2 + \sigma_2^2) t^2}{2})$ 	|




\
\
\
\
\
\
\
  
  
  
  
  
  
  
  
  