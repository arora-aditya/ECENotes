---
title: 'Chapter 5 - Continuous Random Variables<br />'
created: '2019-03-2712:15:00.000Z'
author: 'Aditya Arora'
---

**Definition**: $X$ is called a continuous random variables if there is a non-negative function $f(x)$ such that for any set $B \subset R$

$$P\{X \epsilon B\} = \int_Bf(x)dx$$

$f(x)$ is called the PDF


![$B = [a, b], P\{X\epsilon B\} = \int_a^bf(x)dx$](../../attachments/ch5_pdf.png)

$$
\begin{aligned}
\int_{-\infty}^{\infty}f(x)dx = P\{-\infty < X < \infty\} = 1\\
P\{X = a\} = P\{X \epsilon [a, a]\} = \int_{-a}^{a}f(x)dx = 0
\end{aligned}
$$


**Definition:** Cumulative Distribution Function (CDF)
$$F(a) = P\{X \leq a\} = P\{X < a\} = \int_{-\infty}^{a}f(x)dx$$
$$f(a) = F'(a) = \frac{d}{da}F(a)$$

## Expectation and Variance of Continuous Random Variable

Discrete: $E[X] = \sum_x xP\{X = x\}$

Continuous: $E[X] = \int_{-\infty}^{\infty} xf(x)$

Similarly, $E[aX + b] = aE[X] + b$ where $X$ is a continuous random variable

$$
\begin{aligned}
Var(X) &= E[(X-\mu)^2], \mu = E[x]\\
&= E[X^2] - (E[X])^2\\
Var(aX + b) &= a^2Var(X)
\end{aligned}
$$


## The Uniform Random Variable
A random variable is said to be uniformly distributed over the interval $(0,1)$ if its PDF is 

![uniform random variable](../../attachments/ch5_uniform_rv.png)

$$
f(x) = 
\begin{cases}
1 & 0 < x < 1\\
0 & \text{otherwise}
\end{cases}
$$

In general, a random variable uniformly distributed over $(\alpha, \beta)$ if its PDF is 
$$
f(x) = 
\begin{cases}
\frac{1}{\beta-\alpha} & \text{if }\alpha < x < \beta\\
0 & \text{otherwise}
\end{cases}
$$

Its CDF 
$$
\begin{aligned}
F(a) &= \int_{-\infty}^af(x)dx\\
&=
\begin{cases}
0 & a \leq \alpha\\
\frac{a-\alpha}{\beta-\alpha} & \text{if }\alpha < a < \beta\\
1 & a \geq \beta
\end{cases}
\end{aligned}
$$

![uniform random variable](../../attachments/ch5_uniform_rv_2.png)

$$
\begin{aligned}
E[X] &= \int_{-\infty}^{\infty}xf(x)dx\\
&= \int_{\alpha}^{\beta}x\frac{1}{\beta-\alpha}dx\\ 
&=  \frac{1}{2}\frac{\beta^2-\alpha^2}{\beta-\alpha}\\
&=  \frac{\beta+\alpha}{2}\\
E[X^2] &= \int_{-\infty}^{\infty}x^2f(x)dx\\
&= \int_{\alpha}^{\beta}x^2\frac{1}{\beta-\alpha}dx\\ 
&=  \frac{1}{3}\frac{\beta^3-\alpha^3}{\beta-\alpha}\\
&=  \frac{\beta^2+ \beta\alpha+\alpha^2}{3}\\
Var(X) &= E[X^2] - (E[X])^2\\
&= \frac{\beta^2+ \beta\alpha+\alpha^2}{3} - \frac{\beta^2+\alpha^2 + 2\beta\alpha}{4}\\
&= \frac{(\beta-\alpha)^2}{12}\\
\end{aligned}
$$

## Normal Random Variables

$X$ is a normal (gaussian) random variable if its PDF is

$$f(x)  = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, -\infty < x < \infty$$

where $\mu$ and $\sigma^2$ are the parameters and we can show in fact $\mu$ is the expectation and $\sigma^2$ is the variance of the normal random variable


![Bell Shaped Distribution](../../attachments/ch5_bell_shaped_distribution.png)

Usually, we use $X \sim N(\mu, \sigma^2)$ to denote that $X$ is a normal random variable with parameters $\mu$ and $\sigma^2$

Now, to show that $f(x)$ is indeed a PDF, we need to show that

$$\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx = 1$$

By making the substitutions, $y = \frac{x - \mu}{\sigma}$ we see that

$$\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{\infty}e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{y^2}{2}}dy$$ denoted as $I$

Then, to show $I = 1$ is equivalent to show $I^2 = 1$

$$
\begin{aligned}
I^2 &= \int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{y^2}{2}}dy \cdot \int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx\\
&= \frac{1}{{2\pi}}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-\frac{y^2 + x^2}{2}}dy dx\\
&= \frac{1}{{2\pi}}\int_{0}^{\infty}\int_{0}^{2\pi}e^{-\frac{r^2}{2}}rd\theta dr\\
&= \int_{0}^{\infty}e^{-\frac{r^2}{2}}r dr\\
&= -e^{-\frac{r^2}{2}}\Big|_0^{\infty}{}_{}\\
&= 1
\end{aligned}
$$

In order to verify $E[X] = \mu$ and $Var(X) = \sigma^2$, we need to calculate

$E[X] = \int_{-\infty}^{\infty}xf(x)dx$ and $Var(X) = E[(X-\mu)^2] = \int_{-\infty}^{\infty}(x-\mu)^2 f(x)dx$

Let's first consider the special case when $\mu = 0, \sigma^2 = 1$ which is called standard normal distribution. Its PDF is given by

$f(x) = \frac{1}{\sqrt{2\pi}}e^{\frac{-x^2}{2}}dx$ and we can denote it as $Z \sim N(0,1)$

Now,

$$
\begin{aligned}
E[Z] &= \int_{-\infty}^{\infty}x\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx\\
&= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{x^2}{2}}d\Big(\frac{x^2}{2}\Big)\\
&= -\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\Big|_{-\infty}^{\infty}{}_{}\\
&= 0 = \mu\\
Var(Z) &= E[(Z-\mu)^2] = E[Z^2] = \int_{-\infty}^{\infty}xd(e^{-\frac{x^2}{2}})\\
&= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}xe^{-\frac{x^2}{2}}d(\frac{x^2}{2})\\
&= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}xd(-e^{-\frac{x^2}{2}})\\
&= \frac{1}{\sqrt{2\pi}}x(-e^{-\frac{x^2}{2}}\Big|_{-\infty}^{\infty}) - \int_{-\infty}^{\infty}e^{-\frac{x^2}{2}}dx\\
&= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{x^2}{2}}dx\\
&= 1 (= \sigma^2)
\end{aligned}
$$

Now, we are going to consider the general case, where $X \sim N(\mu, \sigma^2)$

In order to show, in the general case $E[X] = \mu$, and $Var(X) = \sigma^2$, we construct another random variable $Y = \sigma Z + \mu$ where $Z \sim N(0, 1)$

Then, $Y = \sigma Z + \mu$ is normally distributed with parameters $\mu$ and $\sigma^2$ i.e. $Y\sim N(\mu, \sigma^2)$

To show this, we know that
$$
\begin{aligned}
F_r(y) &= P(Y \leq y) &= P(\sigma Z + \mu \leq y) &= P(Z \leq \frac{y - \mu}{\sigma}) = F_Z(\frac{y - \mu}{\sigma})\\
\therefore{} f_r(y) &= f_z(\frac{y - \mu}{\sigma})\frac{1}{\sigma} &= \frac{1}{\sigma}\frac{1}{\sqrt{2\pi}}e^{-\frac{(y-\mu)^2}{2\sigma^2}}
\end{aligned}
$$

Besides, it's obvious that $E[Y] = E[\sigma Z + \mu] = \sigma E[Z] + \mu = \mu\;\;\;\;$ ($E[Z] = 0$) 
$Var(Y) = Var(\sigma Z + \mu) = \sigma^2 Var(Z) = \sigma^2\;\;\;\;$ ($Var(Z) = 1$) 

\

## Normal Random Variable

$$X \sim N(\mu, \sigma^2)$$

$$f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},\;\; E[X] = \mu, Var(X) = \sigma^2$$

Standard $Z \sim N(0, 1)$

$$f(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$$

Distribution Function $F(x) = P\{X \leq x\} = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y-\mu)^2}{{2\sigma^2}}}dy$


Distribution function for standard normal random variable

$$\phi(x) = \int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}e^{\frac{y^2}{2}}dy$$

Th values $\phi(x)$ for non-negative $x$ are given in the table 5.1 in textbook. Because of symmetry, the table only provides $\phi(x)$ for non-negative $x$

![Table 5.1](../../attachments/ch05_table51.png)


Obviously, $\phi(-x) = 1-\phi(x)$, since $\underbrace{P\{X \leq -x\}}_{\phi(-x)} = \underbrace{P\{X \geq x\}}_{1 - \phi(x)}$ for standard normal deviation 

![Normal Random Variable Distribution](../../attachments/ch5_normal_rv.png)


In general case,
$$F(x) = P\{X \leq x\} = P\{\sigma Z + \mu \leq x\} = P\{Z \leq\frac{x-\mu}{\sigma}\} =\phi(\frac{x-\mu}{\sigma})$$

## Exponential Random Variables

An exponential random variable with parameter $\lambda > 0$, has the PDF 
$$f(x) = \begin{cases}
\lambda e^{-\lambda x} & \text{if } x \geq 0\\
0 & \text{if } x < 0\\
\end{cases}$$

$F(x) = 1 - e^{-\lambda x}$ for $x \geq 0$

$E[X] = \frac{1}{\lambda}$, $Var(X)=\frac{1}{\lambda^2}$

## The Gamma Distribution

A gamma distribution with parameters $(\alpha, \lambda)$, $\alpha > 0$ and $\lambda > 0$ has PDF

$$f(x) = \begin{cases}
\frac{\lambda e^{-\lambda x}(\lambda x)^{\alpha - 1}}{\Gamma(\alpha)} & \text{if } x \geq 0\\
0 & \text{if } x < 0\\
\end{cases}$$

where $\Gamma(\alpha) = \int_{0}^{\infty}e^{-y}y^{\alpha - 1}dy$, if $\alpha = n, \Gamma(n) = (n-1)!$

$E[X] = \frac{\alpha}{\lambda}$, $Var(X)=\frac{\alpha}{\lambda^2}$
\
\
\
\
\
\
\
  
  
  
  
  
  
  
  
  