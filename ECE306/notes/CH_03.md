---
title: 'Chapter 3 - Conditional Probability and Independence<br />'
created: '2019-01-0913:52:00.000Z'
author: 'Aditya Arora'
---
## Introduction

#### Two Dice

|Probability|$\frac{1}{36}$|$\frac{2}{36}$|$\frac{3}{36}$|$\frac{4}{36}$|$\frac{5}{36}$|$\frac{6}{36}$|$\frac{5}{36}$|$\frac{4}{36}$|$\frac{3}{36}$|$\frac{2}{36}$|$\frac{1}{36}$|
|-|-|-|-|-|-|-|-|-|-|-|-|
|Sum of 2 dice|2|3|4|5|6|7|8|9|10|11|12|

If the first dice is 6

|Probability|$\frac{1}{36}$|$\frac{1}{36}$|$\frac{1}{36}$|$\frac{1}{36}$|$\frac{1}{36}$|$\frac{1}{36}$|
|-|-|-|-|-|-|-|
|Sum of 2 dice|7|8|9|10|11|12|


## Conditional Probability

$$P(E|F) = \frac{P(EF)}{P(F)}$$

The conditional probability that $E$ occurs given that $F$ has occurred

![ch3_conditional_probability](../../attachments/ch3_conditional_probability.png)

$$P(E) = \frac{\text{number of outcomes in }E}{\text{total number of outcomes in }S}$$

![ch3_conditional_probability_2](../../attachments/ch3_conditional_probability_2.png)

$$P(E|F) = \frac{P(EF)}{P(F)}$$

If the event $F$ occurs, then in order for $E$ to occur it is necessary that the actual occurrence for a point in both $E$ and $F$, that is it must be in $EF$. Now, as $F$ has occurred, $F$ becomes our new or reduced sample space. Thats why $P(E|F) = \frac{P(EF)}{P(F)}$

**Generalize**: Conditional on 2 events

$$P(E|F_1F_2) = \frac{P(EF_1F_2)}{P(F_1F_2)}$$

The multiplication rule

$$P(E_1E_2\dots E_n) = P(E_1)P(E_2|E_1)P(E_3|E_1E_2)\dots P(E_n|E_1\dots E_{n-1})$$

**Proof**: 
$$\begin{aligned}
P(E_1E_2\dots E_n) &= P(E_1)P(E_2|E_1)P(E_3|E_1E_2)\dots P(E_n|E_1\dots E_{n-1})\\
&= P(E_1)\frac{P(E_1E_2)}{P(E_1)}\frac{P(E_1E_2E_3)}{P(E_1E_2)}\dots\frac{P(E_1E_2\dots E_n)}{P(E_1\dots E_{n-1})}\\
&= P(E_1E_2\dots E_n)
\end{aligned}$$

## Bayes' Formula

![ch3_bayes](../../attachments/ch3_bayes.png)

Consider any 2 events $E, F$
$$\begin{aligned}
E &= (EF)\cup (EF^c)\\
P(E) &= P(EF) + P(EF^c)\\
&= P(E|F)P(F) + P(E|F^c)P(F^c)\\
&= P(E|F)P(F) + P(E|F^c)(1 - P(F))\\
\end{aligned}$$

**Definition**: The odds of an event $A$ is defined by 
$$\frac{P(A)}{P(A^c)} = \frac{P(A)}{1 - P(A)}$$


**Generalize**: Consider a hypothesis $H$ that is true with probability $P(H)$ and new evidence $E$ is introduce. The new odds after evidence E has been introduced is: 
$$\frac{P(H|E)}{P(H^c|E)} = \frac{P(H)}{P(H^c)}\frac{P(E|H)}{P(E^c|H)}$$

**Proof**: 

$$\frac{P(EH)}{P(EH^c)} = \frac{P(H)}{P(H^c)}\frac{P(E|H)}{P(E|H^c)}$$
and also
$$\frac{P(EH)}{P(EH^c)} = \frac{P(E)}{P(E)}\frac{P(H|E)}{P(H^c|E)}$$
Thus, $$\frac{P(H|E)}{P(H^c|E)} = \frac{P(H)}{P(H^c)}\frac{P(E|H)}{P(E^c|H)}$$

**Generalize**: Suppose that $F_1, F_2, F_3 \dots F_n$ are mutually exclusive events such that 
$$\bigcup_{i=1}^{n}F_i = S$$
then, $$E = \bigcup_{i=1}^{n}EF_i$$
i.e.
$$P(E) = \sum_{i=1}^{n}P(EF_i) = \sum_{i=1}^nP(F_i)P(E|F_i)$$

![ch3_e_given_fs_cover_S](../../attachments/ch3_e_given_fs_cover_S.png)

### Bayes' Formula

$$P(F_j|E) = \frac{P(EF_j)}{P(E)} = \frac{P(E|F_j)P(F_j)}{\sum_{i=1}^{n}P(E|F_j)P(F_j)}$$


## Independent Events

We have generally seen speaking $P(E|F) \neq P(E)$ but sometimes $P(E|F) = P(E)$

If $P(E|F) = P(E)$ then we say that $E$ and $F$ are independent.

Since $P(E|F) = \frac{P(EF)}{P(F)}$
Thus 
$$
\begin{aligned}
\text{independence} &\Leftrightarrow P(EF) = P(EF)\\
&\Leftrightarrow P(F|E) = P(F)\\
&\Leftrightarrow P(E|F) = P(E)\\
\end{aligned}
$$

Independence is mutual

### Proposition 4.1
If $E$ and $F$ are independent, then so are $E$ and $F^c$

**Proof**: First $E = (EF) \cup (EF^c)$,
Thus, $$
\begin{aligned}
P(E) &= P(EF) + P(EF^c)\\
     &= P(E)P(F) + P(EF^c)\\
P(E)(1 - P(F)) &= P(EF^c)\\
P(E)P(F^c) &= P(EF^c)
\end{aligned}$$
Therefore, $E$ and $F^c$ are independent

Things become surprising if there are 3 events: $E, F, G$

Suppose $E$ is independent of $F$; $E$ is independent of $G$, then is $E$ independent of $FG$

_Answer_: Generally $E$ is not independent of $FG$

**Definition**: Three events $E, F, G$ are said to be independent if 
$$
\begin{aligned}
P(EF) &= P(E)P(F)\\
P(EG) &= P(E)P(G)\\
P(FG) &= P(F)P(G)\\
P(EFG) &= P(E)P(F)P(G)
\end{aligned}
$$

he definition ensures that $P(E|FG) = P(E)$, since 
$$\begin{aligned}
P(E|FG) &= \frac{P(EFG)}{P(FG)}\\
&= \frac{P(E)P(F)P(G)}{P(F)P(G)}\\
&= P(E)
\end{aligned}
$$ 

Also we can obtain $P(E|F\cup G) = P(E)$. What's more, we can show that $E$ is independent of any combination of 2 events $F$ and $G$

**Generally**, $E_1, E_2, \dots E_n$ are independent of 
$$P(E_{i_1},E_{i_2},\dots E_{i_r},) = P(E_{i_1})P(E_{i_2})\cdots P(E_{i_r})$$
for any subset $\{i_1, i_2, \dots i_r \} \subset \{1, 2, 3 \dots n\}$


\
\
\
\
\
\
\
  
  
  
  
  
  
  
  
  