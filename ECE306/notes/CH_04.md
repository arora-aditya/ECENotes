---
title: 'Chapter 4 - Random Variables<br />'
created: '2019-03-0512:15:00.000Z'
author: 'Aditya Arora'
---

## Random Variables

Frequently, we are mainly interested in some function of the outcome than the outcome itself. These real valued functions defined on the sample space are known as random variables

## Discrete Random Variables

A random variable that can take on at most a countable number of possible values is said to be discrete.

For a discrete random variable $X$ we define the probability mass function (PMF) $P(a)$ of $X$ by

$$P(a) = P\{X=a\}$$

If $X$ must assume one of values $\{x_1, x_2, \dots\}$ then 

$$
\begin{aligned}
p(x_i) &\geq 0\;\;&i = 1, 2, \dots\\
p(x) &= 0\;\;&\text{for all other values of } x\\
\sum_{i=1}^{\infty} p(x_i) &= 1
\end{aligned}
$$

**Definition**: The cumulative distribution function (CDF)  $F$ can be expressed in term of $p(x)$ by
$$F(a) = \sum_{\text{all } x \leq a} P(x)$$

If $X$ is a discrete random variable, then its cumulative distribution function is a step function

For example, if $X$ has PMF $p(1) = \frac{1}{4}, p(2) = \frac{1}{2}, p(3) = \frac{1}{8}, p(4) = \frac{1}{8}$ then according to the definition 
$$F(a) = \begin{cases}
0 & a < 1\\
\frac{1}{4} & 1 \leq a < 2\\
\frac{3}{4} & 2 \leq a < 3\\
\frac{7}{8} & 3 \leq a < 4\\
1 & a \geq 4
\end{cases}
$$

It is a right continuous but not a left continuous function

## Expected Value

if $X$ is a discrete random variable having a PMF $p(x)$, then the expectation or expected value of X is 
$$E[x] = \sum_{x:p(x) > 0}x\cdot p(x)$$

The concept of expectation is analogous to the physical concept of he centre of mass.

**Definition:** An indicator variable for the event $A$ is $$I_A = \begin{cases}
1 & \text{if } A \text{ occurs}\\
0 & \text{if } A^c \text{ occurs}\\
\end{cases}$$

$$E[I_A] = 1.p(1) + 0.p(0) = 1.p(A) + 0.p(A^c) = p(A)$$

## Expectation of a function of a random variable

Suppose that we are given a a random variable with its PMF, and we want to compute the expected value fo some function of $X$ say $g(X)$. Equivalently we want to compute the value of $E[g(X)]$


We have known that 
$$
\begin{aligned}
E[X] = \sum_{x:p(x) > 0}xp(x)\\
E[g(X)] = \sum_{x:p(x) > 0}g(x)p(x)\\
\end{aligned}
$$

**Corollary:** If $a, b$ are constant and $X$ is a random variable then $E[aX + b] = aE[X] + b$

**Proof:** $E[aX + b] = \sum_{x: p(x) > 0}(ax + b)p(x) = a\sum_{x: p(x) > 0}x\cdot p(x) + b \sum_{x: p(x) > 0}p(x)$



**Definition:** $E[X]$ is also called the mean, or the first moment of the $X$.

$$E[X^n] = \sum_{x:p(x)> 0}x^np(x)$$ is called the $n^{th}$ moment of $X$

## Variance

$E[X]$ gives the mean, expected value

$$Y = \begin{cases} 
1 & \text{with probability }\frac{1}{2}\\
-1 & \text{with probability }\frac{1}{2}
\end{cases}
$$

$E[Y] = 0$

$$Z = \begin{cases} 
-100 & \text{with probability }\frac{1}{2}\\
-100 & \text{with probability }\frac{1}{2}
\end{cases}
$$

$E[Z] = 0$

For random variables, $Y$ and $Z$ they have the same mean but they behave differently. 

To measure the error, we use $E[|X - \mu|], \mu = E[X]$

Instead, we can use $Var(X) = E[(X - \mu)^2]$ called the variance of $X$

$$
\begin{aligned}
Var(X) &= E[(X - \mu)^2s]\\
&= \sum_{x:p(x) > 0} (x-\mu)^2p(x)\\
&= \sum_{x:p(x) > 0} (x^2+\mu^2 - 2\mu x)p(x)\\
&= \sum_{x:p(x) > 0} x^2p(x)+\mu^2\sum_{x:p(x) > 0}p(x) - 2\mu \sum_{x:p(x) > 0}xp(x)\\
&= E[X^2] - 2\mu\cdot\mu + \mu^2\\
&= E[X^2] - \mu^2
\end{aligned}
$$


**Corollary:** If $a, b$ are constant and $X$ is a random variable then $Var(aX + b) = a^2E[X]$

**Proof:** $$
\begin{aligned}
Var[aX + b] &= E[(aX + b - E[aX +b])^2]\\
&= E[(aX + b - aE[X] - b)^2]\\
&= a^2E[(X - E[X])^2]\\
&= a^2Var[X]
\end{aligned}
$$

**Definition:** The standard deviation $SD(X) = \sqrt{Var(X)}$

## The Bernoulli and Binomial random variables

Bernoulli random variable (success and failure)
$$
X = 
\begin{cases}
1 & \text{with probability } p\\
0 & \text{with probability } 1-p
\end{cases}
$$

$PMF: p(0) = 1-p$, $p(1) = p$

$E[X] = p$; $Var[X] = E[X^2] - (E[X])^2 = p-p^2$ since $E[X^2] = 0^2p(0) + 1^2p(1) = p$ 


Now suppose that we do $n$ independent trials, (each time success with probability $p$; failure with probability $1-p$). Let $X$ represent the number of successes in the $n$ trials. Then $X$ is said to be binomial random variable with parameters (n.p.) $(1,p) \to$ Bernoulli

$PMF: P(i) = {n\choose i}p^i(1-p)^{n-i}$ where $0 \leq i \leq n$
$$\sum_{i=0}^{n}P(i) = \sum_{i=0}^{n}{n\choose i}p^i(1-p)^{n-i} =^{binomial} 1^n = 1$$

The $k^{th}$ moment
$E[X^k] = \sum_{i=0}^{n}i^k{n\choose i}p^i(1-p)^{n-i} = \sum_{i=0}^{n}i^{-k}{n\choose i}p^i(1-p)^{n-i}$

Using identity: $i{n\choose i} = n{n-1}\choose {i-1}$

Then $$\begin{aligned}
E[X^k] &= \sum_{i=0}^{n}i^{-k-1}n{n-1}\choose {i-1}p^i(1-p)^{n-i}\\
&= n\cdot p\cdot \sum_{i=0}^{n}i^{k-1}{n-1}\choose {i-1}p^{i-1}(1-p)^{n-i}\\
&= n\cdot p\cdot \sum_{j=0}^{n-1}(j+1)^{k-1}{n-1}\choose {j}p^{j}(1-p)^{n-1-j}\\
&= n\cdot p\cdot \sum_{j=0}^{n-1}(j+1)^{k-1}p(j)\\
&= n\cdot p\cdot E[(Y+1)^{k-1}] & Y \sim (n-1, p)\\
\end{aligned}
$$

Therefore, $E[X^k] = n\cdot p E[(Y+1)^{k-1}]$, where $Y$ is a binomial random variable with parameters $(n-1, p)$

Now, let $k = 1$, $E[X] = npE[(Y+1)^{k-1}] = npE[1] = np$

let $k = 2$, $E[X^2] = npE[(Y+1)^{2-1}] = npE[Y+1] = np[E[Y] +1] = np[(n-1)p + 1]$

Thus, $Var(X) = E[X^2] - (E[X])^2 = n(n-1)p^2  +np - (np)^2 = np(1-p)$

$\implies Var(Y) = (n-1)p(1-p)$

If $z \sim Binomial(k, q)$, then $E[Z] = kq, Var(Z) = kq(1-q)$

Its PMF has the shape as follows: 

If $X$ is a binomial random variable $\sim (n, p), 0 < p < 1$

![$i^*$ is the largest integer less than or equal to $(n+1)p$](../../attachments/ch4_bernoulli.png)


## The Poisson Random Variable

A random variable $X$ on one of th values $0,1,2,\dots$ is said to b a Poisson random variable with parameter $\lambda$ if for some $\lambda > 0$, its PMF $$P(i) = e^{-\lambda}\frac{\lambda^i}{i!}, i = 0,1,2,\dots$$

To verify $\sum_{i=0}^{\infty}P(i) = 1$, we have 
$$\begin{aligned}
& \sum_{i=0}^{\infty}P(i)\\
\implies & \sum_{i=0}^{\infty}e^{-\lambda}\frac{\lambda^i}{i!}\\
\implies & e^{-\lambda}\sum_{i=0}^{\infty}\frac{\lambda^i}{i!}\\
\implies & e^{-\lambda}e^{\lambda} = 1
\end{aligned}$$


![parameter $\lambda$ affects the slope of the PMF](../../attachments/ch4_poisson_variable.png)

$$
\begin{aligned}
E[X] &= \sum_{i=0}^{\infty}\frac{ie^{-\lambda}\lambda^i}{i!}\\
&= \sum_{i=1}^{\infty}\frac{ie^{-\lambda}\lambda^i}{i!}\\
&= \lambda\sum_{i=1}^{\infty}\frac{e^{-\lambda}\lambda^{i-1}}{(i-1)!}\\
&=^{j=i-1} \lambda\sum_{j=0}^{\infty}\frac{e^{-\lambda}\lambda^{j}}{j!}\\
&= \lambda
\end{aligned}
$$

$$
\begin{aligned}
E[X^2] &= \sum_{i=0}^{\infty}\frac{i^2e^{-\lambda}\lambda^i}{i!}\\
&= \sum_{i=1}^{\infty}\frac{i^2e^{-\lambda}\lambda^i}{i!}\\
&= \lambda\sum_{i=1}^{\infty}i\frac{e^{-\lambda}\lambda^{i-1}}{(i-1)!}\\
&=^{j=i-1} \lambda\sum_{j=0}^{\infty}(j+1)\frac{e^{-\lambda}\lambda^{j}}{j!}\\
&=\lambda\Big[\sum_{j=0}^{\infty}j\frac{e^{-\lambda}\lambda^{j}}{j!} + \sum_{j=0}^{\infty}\frac{e^{-\lambda}\lambda^{j}}{j!}\Big]\\
&= \lambda(\lambda + 1)\\
&= \lambda^2 + \lambda
\end{aligned}
$$

Thus, $Var(X) = E[X^2] - E[X]^2 = \lambda^2 + \lambda - \lambda^2 = \lambda$

## The Geometric Random Variables

Independent trials until a success occurs, the $\#$ of times we do the trials is a geometric random variables.

PMF: $P(i) = (1-p)^{i-1}p$
$$
\begin{aligned}
\sum_{i=1}^{\infty}P(i) &= \sum_{i=1}^{\infty}(1-p)^{i-1}p\\
&= p\sum_{i=1}^{\infty}(1-p)^{i-1}\\
&= p\frac{1}{1 - (1-p)}\\
&= 1
\end{aligned}
$$

$E[X] = \frac{1}{p}, Var(X) = \frac{1-p}{p^2}$


\
\
\
\
\
\
\
  
  
  
  
  
  
  
  
  